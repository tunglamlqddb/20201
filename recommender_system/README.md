Please follow this drive for detailed source code and dataset
https://drive.google.com/drive/folders/1PCebN4wiW744kH6n_b-N0fJ8p1WEtehq?usp=sharing

**LOTUS VIDEO RECOMMENDATION SYSTEM**

- This project implements two approaches for video recommendation, Content-based and Collaborative Filtering based (using Variational Auto-Enconder) on a social platform dataset given by Lotus. 

- The raw data can be found in this link: https://drive.google.com/drive/u/8/folders/1xh7b2W3huBBo7yiFNuosxV_RKw3_m4rt

- **Data preprocess:** Videos in data are given along with their titles.
  - **Video preprocess** (*pre_process/embedding_video/preprocess_video.ipynb*). Firstly,download videos from given links. Secondly, for each video, extract first 40 frames, pass through VGG16, then get the flattened featured map. Finally, take the average vector of these 40 flattened vectors, this is a video representation vector. These vectors are saved at (*pre_process/embedding_video/vector*).
  - **Title preprocess** (*pre_process/embedding_sentence/Embedding sentence.ipynb*).
Firstly, remove characters that are not numbers of words. Secondly, tokenize the sentence, create a dictionary with document-frequency (df) thresh is 15. Finally, save them in (*model/Content Base/vocab.txt*).

- **Visualization:** Some statistic and visualization could be found at folder *pre_preprocess*.

- **Content-based model**: This model is based on the paper *Large-Scale Content-Only Video Recommendation* (https://ieeexplore.ieee.org/document/8265330).
  - Traing data is generated by creating **co-watch** pairs of videos. 
  - The processed title and video vectors above are first concatenated, then passed through a simple MLP network to finetune and get a feature vector related more to the dataset. Triplet loss is used for this finetune phase.
  - When predicting, user representation is constructed (taking average) from his/her watched video representations. Then the video with highest similarity is the final prediction.
  - Achieved 0.16 NDCG@5, 0.23 Recall@5, 0.25 NDCG@10, 0.43 Recall@10.

- **VAE-CF model**: This model is based on the paper *Variational Autoencoders for Collaborative Filtering* (https://arxiv.org/abs/1802.05814).
  - | K   | NDCG   | Recall
    | --- | -----  | ------ |
    | 10  | 0.1366 | 0.2089 |
    | 20  | 0.1617 | 0.2874 |
    | 50  | 0.1946 | 0.4154 |
    | 100 | 0.2176 | 0.5234 |
